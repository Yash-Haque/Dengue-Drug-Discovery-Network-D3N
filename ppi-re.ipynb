{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7285796,"sourceType":"datasetVersion","datasetId":4224916}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/BNLNLP/PPI-Relation-Extraction.git","metadata":{"execution":{"iopub.status.busy":"2023-12-27T18:14:06.742906Z","iopub.execute_input":"2023-12-27T18:14:06.743630Z","iopub.status.idle":"2023-12-27T18:14:11.336803Z","shell.execute_reply.started":"2023-12-27T18:14:06.743594Z","shell.execute_reply":"2023-12-27T18:14:11.335854Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Cloning into 'PPI-Relation-Extraction'...\nremote: Enumerating objects: 899, done.\u001b[K\nremote: Counting objects: 100% (15/15), done.\u001b[K\nremote: Compressing objects: 100% (9/9), done.\u001b[K\nremote: Total 899 (delta 7), reused 14 (delta 6), pack-reused 884\u001b[K\nReceiving objects: 100% (899/899), 20.93 MiB | 14.43 MiB/s, done.\nResolving deltas: 100% (499/499), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -r /kaggle/working/PPI-Relation-Extraction/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-12-27T18:14:11.338619Z","iopub.execute_input":"2023-12-27T18:14:11.338919Z","iopub.status.idle":"2023-12-27T18:14:13.658287Z","shell.execute_reply.started":"2023-12-27T18:14:11.338893Z","shell.execute_reply":"2023-12-27T18:14:13.657183Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.2 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.2\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Weights and Biases (wandb) installation and login","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2023-12-27T18:14:18.197754Z","iopub.execute_input":"2023-12-27T18:14:18.198667Z","iopub.status.idle":"2023-12-27T18:14:29.623496Z","shell.execute_reply.started":"2023-12-27T18:14:18.198620Z","shell.execute_reply":"2023-12-27T18:14:29.622577Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# Running inference","metadata":{}},{"cell_type":"code","source":"%cd \"/kaggle/working/\"","metadata":{"execution":{"iopub.status.busy":"2023-12-26T09:58:05.006922Z","iopub.execute_input":"2023-12-26T09:58:05.007336Z","iopub.status.idle":"2023-12-26T09:58:05.015690Z","shell.execute_reply.started":"2023-12-26T09:58:05.007303Z","shell.execute_reply":"2023-12-26T09:58:05.014630Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"!python run_re.py \\\n  --model_list dmis-lab/biobert-base-cased-v1.1 \\\n  --task_name \"re\" \\\n  --dataset_dir \"/kaggle/working/PPI-Relation-Extraction/datasets\" \\\n  --dataset_name \"/kaggle/working/PPI-Relation-Extraction/datasets/PPI/original/HPRD50\" \\\n  --output_dir \"/kaggle/working/PPI-Relation-Extraction/Outputs\" \\\n  --do_train \\\n  --do_predict \\\n  --seed 1 \\\n  --remove_unused_columns False \\\n  --save_steps 100000 \\\n  --per_device_train_batch_size 16 \\\n  --per_device_eval_batch_size 32 \\\n  --num_train_epochs 10 \\\n  --optim \"adamw_torch\" \\\n  --learning_rate 5e-05 \\\n  --warmup_ratio 0.0 \\\n  --weight_decay 0.0 \\\n  --relation_representation \"EM_entity_start\" \\\n  --use_context \"attn_based\" \\\n  --do_cross_validation True \\\n  --overwrite_cache \\\n  --overwrite_output_dir","metadata":{"execution":{"iopub.status.busy":"2023-12-27T18:34:25.672303Z","iopub.execute_input":"2023-12-27T18:34:25.672692Z","iopub.status.idle":"2023-12-27T18:34:36.910757Z","shell.execute_reply.started":"2023-12-27T18:34:25.672661Z","shell.execute_reply":"2023-12-27T18:34:36.909606Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n\n\n Initial Output Dir at first instance:/kaggle/working/PPI-Relation-Extraction/Outputs\n\n\n\n\n\n\n Initial Output Dir in for loop:/kaggle/working/PPI-Relation-Extraction/Outputs\n\n\n\n*** No. of datasets: 1 ***\n*** No. of datasets: 2 ***\n*** No. of datasets: 3 ***\n*** No. of datasets: 4 ***\n*** No. of datasets: 5 ***\n*** No. of datasets: 6 ***\n*** No. of datasets: 7 ***\n*** No. of datasets: 8 ***\n*** No. of datasets: 9 ***\n*** No. of datasets: 10 ***\n[INFO|configuration_utils.py:715] 2023-12-27 18:34:33,423 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/config.json\n[INFO|configuration_utils.py:775] 2023-12-27 18:34:33,425 >> Model config BertConfig {\n  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"finetuning_task\": \"re\",\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"output_attentions\": true,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.33.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\n[INFO|tokenization_auto.py:535] 2023-12-27 18:34:33,524 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n[INFO|configuration_utils.py:715] 2023-12-27 18:34:33,623 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/config.json\n[INFO|configuration_utils.py:775] 2023-12-27 18:34:33,624 >> Model config BertConfig {\n  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.33.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\n[INFO|tokenization_utils_base.py:1852] 2023-12-27 18:34:33,826 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/vocab.txt\n[INFO|tokenization_utils_base.py:1852] 2023-12-27 18:34:33,826 >> loading file tokenizer.json from cache at None\n[INFO|tokenization_utils_base.py:1852] 2023-12-27 18:34:33,826 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1852] 2023-12-27 18:34:33,826 >> loading file special_tokens_map.json from cache at None\n[INFO|tokenization_utils_base.py:1852] 2023-12-27 18:34:33,826 >> loading file tokenizer_config.json from cache at None\n[INFO|configuration_utils.py:715] 2023-12-27 18:34:33,826 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/config.json\n[INFO|configuration_utils.py:775] 2023-12-27 18:34:33,827 >> Model config BertConfig {\n  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.33.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\n[INFO|configuration_utils.py:715] 2023-12-27 18:34:33,860 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/config.json\n[INFO|configuration_utils.py:775] 2023-12-27 18:34:33,861 >> Model config BertConfig {\n  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.1\",\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.33.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 28996\n}\n\n[INFO|tokenization_utils_base.py:926] 2023-12-27 18:34:33,874 >> Assigning ['[E1]', '[/E1]', '[E2]', '[/E2]', '[E1-E2]', '[/E1-E2]'] to the additional_special_tokens key of the tokenizer\n[INFO|modeling_utils.py:2857] 2023-12-27 18:34:33,874 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--dmis-lab--biobert-base-cased-v1.1/snapshots/924f12e0c3db7f156a765ad53fb6b11e7afedbc8/pytorch_model.bin\n[INFO|modeling_utils.py:3633] 2023-12-27 18:34:35,279 >> Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.1 were not used when initializing BertForRelationClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForRelationClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForRelationClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:3645] 2023-12-27 18:34:35,279 >> Some weights of BertForRelationClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n[WARNING|modeling_utils.py:1507] 2023-12-27 18:34:35,301 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 29002. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n\nTrain File: 0. Test File: 0\n\nTraceback (most recent call last):\n  File \"/kaggle/working/PPI-Relation-Extraction/src/relation_extraction/run_re.py\", line 767, in <module>\n    main()\n  File \"/kaggle/working/PPI-Relation-Extraction/src/relation_extraction/run_re.py\", line 520, in main\n    data_files = read_dataset(dataset_num, task_name, data_args)\n  File \"/kaggle/working/PPI-Relation-Extraction/src/relation_extraction/dataset_utils.py\", line 49, in read_dataset\n    sorted_data = sorted(data)\nTypeError: '<' not supported between instances of 'dict' and 'dict'\n","output_type":"stream"}]},{"cell_type":"code","source":"!wget /kaggle/working/PPI-Relation-Extraction/datasets/PPI/original/HPRD50/dmis-lab/biobert-base-cased-v1.1/EM_entity_start_ac/pytorch_model.bin","metadata":{"execution":{"iopub.status.busy":"2023-12-26T09:51:59.671969Z","iopub.execute_input":"2023-12-26T09:51:59.672906Z","iopub.status.idle":"2023-12-26T09:52:00.625598Z","shell.execute_reply.started":"2023-12-26T09:51:59.672867Z","shell.execute_reply":"2023-12-26T09:52:00.624677Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/working/PPI-Relation-Extraction/datasets/PPI/original/HPRD50/dmis-lab/biobert-base-cased-v1.1/EM_entity_start_ac/pytorch_model.bin: Scheme missing.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **--All Hyperparameters--**","metadata":{}},{"cell_type":"code","source":"!mv \"/kaggle/working/PPI-Relation-Extraction/datasets/EU-ADR_BioBERT (train & validation)\" \"/kaggle/working/PPI-Relation-Extraction/datasets/EU-ADR_BioBERT\"","metadata":{"execution":{"iopub.status.busy":"2023-11-30T03:22:29.521681Z","iopub.execute_input":"2023-11-30T03:22:29.522080Z","iopub.status.idle":"2023-11-30T03:22:30.494353Z","shell.execute_reply.started":"2023-11-30T03:22:29.522049Z","shell.execute_reply":"2023-11-30T03:22:30.492873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/PPI-Relation-Extraction/src/relation_extraction","metadata":{"execution":{"iopub.status.busy":"2023-12-27T18:14:36.342191Z","iopub.execute_input":"2023-12-27T18:14:36.343065Z","iopub.status.idle":"2023-12-27T18:14:36.348990Z","shell.execute_reply.started":"2023-12-27T18:14:36.343032Z","shell.execute_reply":"2023-12-27T18:14:36.348099Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working/PPI-Relation-Extraction/src/relation_extraction\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile dataset_utils.py\nimport os\nimport sys\nimport re\nimport pickle\nimport json\nimport pandas as pd\nimport numpy as np\nfrom datasets import ClassLabel, load_dataset, load_metric, Dataset, DatasetDict, concatenate_datasets\nfrom transformers import BertTokenizerFast, RobertaTokenizerFast\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nimport random\n\ndef extract_json_objects(file_path):\n    json_objects = []\n    with open(file_path,\"r\") as file:\n        for line in file:\n            try:\n                json_object = json.loads(line)\n                json_objects.append(json_object)\n            except Exception as e:\n                print(f\"Error parsing line {file.tell() - len(line)}: {e}\")\n    return json_objects\n\ndef read_dataset(dataset_num=0, task_name=None, data_args=None):\n    data_files = {}\n    if data_args.train_file is not None and data_args.test_file is not None:\n        data_files[\"train\"] = data_args.train_file\n        data_files[\"test\"] = data_args.test_file\n        print(f\"\\nTrain File: {data_args.train_file}. Test File: {data_args.test_file}\\n\")\n    else:\n        data_dir = os.path.join(data_args.dataset_dir, data_args.dataset_name)\n        # data_files[\"train\"] = os.path.join(data_dir, 'train_' + str(dataset_num) + '.json')\n        training_data = os.path.join(data_dir, 'train_' + str(dataset_num) + '.json')\n        data_files[\"test\"] = os.path.join(data_dir, 'test_' + str(dataset_num) + '.json')\n        print(f\"\\nTrain File: {dataset_num}. Test File: {dataset_num}\\n\")\n        if os.path.isfile(os.path.join(data_dir, 'dev_' + str(dataset_num) + '.json')):\n            data_files[\"validation\"] = os.path.join(data_dir, 'dev_' + str(dataset_num) + '.json')\n        else:\n            \n            \n            # Open a json file\n            json_objects = extract_json_objects(training_data)\n            data = [item for sublist in json_objects for item in sublist.items()]\n            # Load all the contents to a dictionary\n            sorted_data = sorted(data)\n            \n            # Sort and shuffle the data\n            random.shuffle(sorted_data)\n            \n            # Extract the remaining data into a dictionary, data_files under the key \"validation\"\n            training_data_cutoff = int(0.8 * len(sorted_data))\n            training_data = dict(sorted_data[:training_data_cutoff])\n            validation_data = dict(sorted_data[training_data_cutoff:])\n            \n            data_files[\"train\"] = training_data\n            data_files[\"validation\"] = validation_data\n            \n            print(f\"\\nData Files being sent in for eval set:{data_files['train']}.\\n\")\n            \n    extension = data_files[\"train\"].split(\".\")[-1]\n    print(f\"\\nExtension: {extension}\\n\")\n    print(f\"\\nData Files: {data_files}\\n\")\n    \n    return load_dataset(extension, data_files=data_files)\n    \n    \ndef tokenize_and_set_relation_labels(examples, tokenizer, padding, max_seq_length, relation_representation, use_context):\n\n    if 'text' in examples:\n        if relation_representation.startswith('EM'):\n            token_key = 'text_with_entity_marker'\n        else:\n            token_key = 'text'\n        \n        tokenized_inputs = tokenizer(\n            examples[token_key],\n            padding=padding,\n            truncation=True,\n            max_length=max_seq_length,\n        )\n    elif 'tokens' in examples:\n        if relation_representation.startswith('EM'):\n            token_key = 'tokens_with_marker'\n        else:\n            token_key = 'tokens'\n        \n        tokenized_inputs = tokenizer(\n            examples[token_key],\n            padding=padding,\n            truncation=True,\n            max_length=max_seq_length,\n            # We use this argument because the texts in our dataset are lists of words.\n            is_split_into_words=True,\t\n        )\n    else:\n        raise Exception(\"There is no tokens element in the data!!\")\n\n    labels = []\n    relations = []\n    \n    tokens_seq = []\n    tokens_to_ignore = []\n\n    # Most data has a single relation per example, but some data such as SciERC has multiple relations in a sentence.\n    for i, rel_list in enumerate(examples['relation']):\n        label_ids = []\n        relation_spans = []\n        predicate_spans = []\n        ent_types = []\n        \n        for rel in rel_list:\n            if 'text' in examples:\n                \n                # ref: https://www.lighttag.io/blog/sequence-labeling-with-transformers/example\n                # ref: https://github.com/huggingface/transformers/issues/9326\n                def get_token_idx(char_idx):\n                    while True:\n                        # if it's the last index, return the last token.\n                        if char_idx == len(examples[token_key][i]):\n                            return len(tokenized_inputs[i]) - 1\n                        \n                        token_idx = tokenized_inputs.char_to_token(batch_or_char_index=i, char_index=char_idx)\n                        # Whitespaces have no token and will return None.\n                        if token_idx is not None:\n                            return token_idx\n                        \n                        char_idx += 1\n                        # debug\n                        #if char_idx == len(examples[token_key][i]):\n                        #\traise Exception(\"End token not found: \" f\"{tokenizer.convert_ids_to_tokens(tokenized_inputs['input_ids'][i])}\")\n\n                e1_span_idx_list, e2_span_idx_list = [], []\n                \n                if relation_representation.startswith('EM'):\n                    e1_idx = rel['entity_1_idx_in_text_with_entity_marker']\n                    e2_idx = rel['entity_2_idx_in_text_with_entity_marker']\n                    \n                    ## TODO: remove this!! the first token is used for separate tokens.\n                    if np.asarray(e1_idx).ndim > 1 or np.asarray(e2_idx).ndim > 1:\n                        raise Exception(\"For now, entity marker representations do not support separate entities.\")\n                else:\n                    e1_idx = rel['entity_1_idx']\n                    e2_idx = rel['entity_2_idx']\n                \n                # Some dataset (e.g., BioInfer) has entities consisting of separate tokens.\n                # To match the dimension to separate entities, add a dimension for single entities. \n                e1_idx = [e1_idx] if np.asarray(e1_idx).ndim == 1 else e1_idx\n                e2_idx = [e2_idx] if np.asarray(e2_idx).ndim == 1 else e2_idx\n                \n                for e1_s, e1_e in e1_idx:\n                    e1_span_s = get_token_idx(e1_s)\n                    e1_span_e = get_token_idx(e1_e)\n                    e1_span_idx_list.append((e1_span_s, e1_span_e))\n                \n                for e2_s, e2_e in e2_idx:\n                    e2_span_s = get_token_idx(e2_s)\n                    e2_span_e = get_token_idx(e2_e)\n                    e2_span_idx_list.append((e2_span_s, e2_span_e))\n            \n            entity_1_type_id = rel['entity_1_type_id']\n            entity_2_type_id = rel['entity_2_type_id']\n\n            label_ids.append(rel['relation_id'])\n            relation_spans.extend([e1_span_idx_list, e2_span_idx_list])\n            ent_types.extend([entity_1_type_id, entity_2_type_id])\n            \n        labels.append(label_ids)\n        relations.append(relation_spans)\n\n        if use_context == \"attn_based\":\n            input_tokens = tokenized_inputs.tokens(batch_index=i)\n            \n            entity_indice = []\n            for r_s in relation_spans:\n                for span_s, span_e in r_s:\n                    entity_indice.extend(list(range(span_s, span_e)))\n            \n            entity_indice = list(set(entity_indice))\n\n            tokens_seq.append([1 if tok.startswith('##') else 0 for tok in input_tokens])\n            tokens_to_ignore.append([-100 if re.search('[a-zA-Z0-9]', tok) == None or \\\n                                             #tok in tokenizer.all_special_tokens or \\\n                                             tok in list(set(tokenizer.all_special_tokens) - set(tokenizer.additional_special_tokens)) or \\\n                                             idx in entity_indice \\\n                                          else 0 for idx, tok in enumerate(input_tokens)])\n            \n            tokenized_inputs['tokens_seq'] = tokens_seq\n            tokenized_inputs['tokens_to_ignore'] = tokens_to_ignore\n    \n    tokenized_inputs['labels'] = labels\n    tokenized_inputs['relations'] = relations\n    \n    return tokenized_inputs\n\n\ndef featurize_data(dataset, tokenizer, padding, max_seq_length, relation_representation, use_context):\n    convert_func_dict = tokenize_and_set_relation_labels\n    \n    if use_context == \"attn_based\":\n        columns = ['input_ids', 'attention_mask', 'labels', 'token_type_ids', 'relations', 'tokens_seq', 'tokens_to_ignore']\n    else:\n        columns = ['input_ids', 'attention_mask', 'labels', 'token_type_ids', 'relations']\n\n    features = {}\n    for phase, phase_dataset in dataset.items():\n        features[phase] = phase_dataset.map(\n            convert_func_dict,\n            fn_kwargs={'tokenizer': tokenizer,\n                       'padding': padding,\n                       'max_seq_length': max_seq_length,\n                       'relation_representation': relation_representation,\n                       'use_context': use_context},\n            batched=True,\n            load_from_cache_file=False,\n        )\n        \n        features[phase].set_format(\n            #type=\"torch\",\n            type=None,\n            columns=columns,\n        )\n    \n    return features\n","metadata":{"execution":{"iopub.status.busy":"2023-12-27T18:34:20.234740Z","iopub.execute_input":"2023-12-27T18:34:20.235127Z","iopub.status.idle":"2023-12-27T18:34:20.248538Z","shell.execute_reply.started":"2023-12-27T18:34:20.235097Z","shell.execute_reply":"2023-12-27T18:34:20.247668Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Overwriting dataset_utils.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile run_re.py\nimport os\nimport sys\nimport logging\nimport shutil\nimport json\nimport numpy as np\nfrom dataclasses import dataclass, field\nfrom typing import List, Union, Dict, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data.dataloader import DataLoader\n#from torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.sampler import RandomSampler\n\nimport datasets\nfrom datasets import load_dataset, load_metric\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    BertTokenizerFast,\n    RobertaTokenizerFast,\n    DebertaV2Tokenizer,\n    DebertaTokenizerFast,\n    DataCollatorForTokenClassification,\n    HfArgumentParser,\n    PreTrainedModel,\n    PreTrainedTokenizerFast,\n    PretrainedConfig,\n    Trainer,\n    TrainingArguments,\n    EvalPrediction,\n    set_seed,\n)\nfrom transformers.data.data_collator import DataCollator, InputDataClass\nfrom transformers.trainer_utils import get_last_checkpoint, is_main_process\nfrom transformers.utils import check_min_version\n\nfrom dataset_utils import *\nfrom model import BertForRelationClassification\nfrom data_collator import DataCollatorForRelationClassification\n\n\ndataset_list = [\"ChemProt_BLURB\", \"DDI_BLURB\", \"GAD_BLURB\", \"EU-ADR_BioBERT\",\n                \"AImed\", \"BioInfer\", \"HPRD50\", \"IEPA\", \"LLL\",\n                \"Typed_PPI\",\n                \"P-putida\", \"P-species\"]\n\ndataset_max_seq_length = {\n    #\"ChemProt_BLURB\": 256, # some samples (count: 11) are longer than 256 tokens.\n    #\"DDI_BLURB\": 256, # many samples are longer than 256 tokens. \n    #\"GAD_BLURB\": 128, # some samples (count: 3) are longer than 128 tokens when BioBERT is used.\n    #\"EU-ADR_BioBERT\": 128, # some samples (count: 14) are longer than 128 tokens.\n}\n\ndataset_special_tokens = {\n    \"ChemProt_BLURB\": [\"@GENE$\", \"@CHEMICAL$\", \"@CHEM-GENE$\"],\n    \"DDI_BLURB\": [\"@DRUG$\", \"@DRUG-DRUG$\"],\n    \"GAD_BLURB\": [\"@GENE$\", \"@DISEASE$\"],\n    \"EU-ADR_BioBERT\": [\"@GENE$\", \"@DISEASE$\"],\n}\n\nentity_marker_special_tokens = {\n    \"EM\": [\"[E1]\", \"[/E1]\", \"[E2]\", \"[/E2]\", \"[E1-E2]\", \"[/E1-E2]\"],\n    \"ChemProt_BLURB\": [\"[GENE]\", \"[/GENE]\", \"[CHEM]\", \"[/CHEM]\", \"[CHEM-GENE]\", \"[/CHEM-GENE]\"],\n    \"DDI_BLURB\": [\"[DRUG]\", \"[DRUG-DRUG]\"],\n    \"GAD_BLURB\": [\"[GENE]\", \"[/GENE]\", \"[DISEASE]\", \"[/DISEASE]\"],\n}\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    # [START][GP] - input parameter for a list of models. 04-11-2021\n    model_name_or_path: str = field(\n        default='bert-base-cased',\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    model_list: List[str] = field(\n        default_factory=lambda: ['bert-base-cased', \n                                 'bert-large-cased', \n                                 'dmis-lab/biobert-base-cased-v1.1', \n                                 'dmis-lab/biobert-large-cased-v1.1'],\n        metadata={\"help\": \"a list of models.\"},\n    )\n    # [END][GP] - input parameter for a list of models. 04-11-2021\n\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n            \"with private models).\"\n        },\n    )\n    # [START][GP] - added do_lower_case parameter for tokenizer. 04-07-2021\n    do_lower_case: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to lowercase words or not. Basically, this option follows model's config, \"\n            \"but, some models (e.g., BioBERT cased) needs to be explicitly set.\"\n        },\n    )\n    # [END][GP] - added do_lower_case parameter for tokenizer. 04-07-2021\n\n\n@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n    \n    task_name: Optional[str] = field(default=\"re\", metadata={\"help\": \"The name of the task.\"})\n\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    max_seq_length: int = field(\n        default=512,\n        metadata={\n            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n            \"than this will be truncated, sequences shorter will be padded.\"\n        },\n    )\n    train_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"The input training data file (a JSON file).\"}\n    )\n    validation_file: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a JSON file).\"},\n    )\n    test_file: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"An optional input test data file to predict on (a JSON file).\"},\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    pad_to_max_length: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Whether to pad all samples to model maximum sentence length. \"\n            \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n            \"efficient on GPU but very bad for TPU.\"\n        },\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n            \"value if set.\"\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n            \"value if set.\"\n        },\n    )\n    max_test_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"For debugging purposes or quicker training, truncate the number of test examples to this \"\n            \"value if set.\"\n        },\n    )\n\n    # [START][GP] - data parameters.\n    dataset_dir: Optional[str] = field(\n        default=None, metadata={\"help\": \"The path to the parent directory of all datasets.\"},\n    )\n    relation_types: str = field(\n        default=None, metadata={\"help\": \"Relation type file (name and id).\"},\n    )\n    relation_representation: str = field(\n        default=\"STANDARD_mention_pooling\",\n        metadata={\"help\": \"vairous relation representations from [2019] Matching the Blanks: Distributional Similarity for Relation Learning. \"\n                          \"Largely, the representations are divided into standard and entity markers. \"\n                          \"Options: \"\n                          \"1) standard: STANDARD_cls_token, \"\n                          \"\t\t\t\tSTANDARD_mention_pooling, \"\n                          \"2) entity markers (EM): EM_cls_token, \"\n                          \"\t\t\t\t\t\t   EM_mention_pooling, \"\n                          \"\t\t\t\t\t\t   EM_entity_start, \"\n                          \" * for poolings, max pooling is used. \"},\n    )\n    use_context: str = field(\n        default=None,\n        metadata={\"help\": \"Here, context indicates tokens related to entities' relational information. \"\n                          \"The context is appended to relation representations.  \"\n                          \"Options: \"\n                          \"1) attn_based: context based on attention probability calculation, \"\n                          \"2) local: local context (tokens between the two entities) \"},\n    )\n    # [END][GP] - data parameters.\n\n    # [START][GP] - cross-validation parameters.\n    do_cross_validation: Optional[bool] = field(\n        default=False, \n        metadata={\"help\": \"Whether to use cross-validation for evaluation.\"}\n    )\n    num_of_folds: Optional[int] = field(\n        default=10, \n        metadata={\"help\": \"The number of folds for the cross-validation.\"}\n    )\n    ratio: Optional[str] = field(\n        default='80-10-10', \n        metadata={\"help\": \"train/dev/test ratio: 80-10-10, 70-15-15, 60-20-20\"}\n    )\n    # [END][GP] - cross-validation parameters.\n    \n    save_predictions: Optional[bool] = field(\n        default=False, \n        metadata={\"help\": \"Whether to save predictions along with ground truth labels. It's usually for debugging purpose.\"}\n    )\n    \n    def __post_init__(self):\n        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n        elif self.dataset_name is not None:\n            d_name = self.dataset_name.rsplit('/', 1)[1] if len(self.dataset_name.rsplit('/', 1)) > 1 else self.dataset_name\n            if d_name not in dataset_list:\n                raise ValueError(\"Unknown dataset, you should pick one in \" + \", \".join(dataset_list) + \". Or, add your dataset to the dataset list.\")\n        else:\n            if self.train_file is not None:\n                extension = self.train_file.split(\".\")[-1]\n                assert extension == \"json\", \"`train_file` should be a json file.\"\n            if self.validation_file is not None:\n                extension = self.validation_file.split(\".\")[-1]\n                assert extension == \"json\", \"`validation_file` should be a json file.\"\n            if self.test_file is not None:\n                extension = self.test_file.split(\".\")[-1]\n                assert extension == \"json\", \"`test_file` should be a json file.\"\n\n\ndef main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n\n    # Setup logging\n    # Yashfinul Haque -28/11/23\n#     FORMAT = '%(asctime)s %(clientip)-15s %(user)-8s %(levelname)-8s %(message)s'\n\n#     logging.basicConfig(\n#         format=FORMAT,\n#         datefmt=\"%m/%d/%Y %H:%M:%S\",\n#         level=logging.DEBUG,  # Set the logging level to DEBUG to capture all levels\n#         handlers=[logging.StreamHandler(sys.stdout)],\n#     )\n    \n#     log_level = training_args.get_process_log_level()\n#     logger.setLevel(log_level)\n#     datasets.utils.logging.set_verbosity(log_level)\n#     transformers.utils.logging.set_verbosity(log_level)\n#     transformers.utils.logging.enable_default_handler()\n#     transformers.utils.logging.enable_explicit_format()\n\n    # Setup logging [SOURCE: https://programtalk.com/python-more-examples/transformers.utils.logging.enable_default_handler/]\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        # level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN\n        level=logging.INFO,\n    )\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n    # Set the verbosity to info of the Transformers logger (on main process only):\n    if is_main_process(training_args.local_rank):\n        transformers.utils.logging.set_verbosity_info()\n        \n    # Log on each process the small summary:\n    logger.info(\n        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n    )\n    logger.info(f\"Training/evaluation parameters {training_args}\")\n    #print(f\"\\nTraining/evaluation parameters {training_args}\\n\")\n    \n    relation_representation = data_args.relation_representation\n    \n    model_list = model_args.model_list\n    task_name = data_args.task_name\n    dataset_name = data_args.dataset_name\n    \n    # Load relation and entity types.\n    relation_type_file = os.path.join(data_args.dataset_dir, dataset_name)\n    relation_type_file = os.path.join(relation_type_file, \"relation_types.json\")\n    relation_types = json.load(open(relation_type_file))\n\n    label_list = list(relation_types.keys())\n        \n    initial_output_dir = training_args.output_dir\n    print(f\"\\n\\n\\n Initial Output Dir at first instance:{initial_output_dir}\\n\\n\\n\")\n    \n    if not os.path.exists(initial_output_dir):\n        os.makedirs(initial_output_dir)\n    \n    # Here the author modifies the output_dir to be from being initial_output_dir to output_dir of model_name and relation_representation\n    for model_name in model_list:\n        model_args.model_name_or_path = model_name\n        training_args.output_dir = initial_output_dir\n        print(f\"\\n\\n\\n Initial Output Dir in for loop:{initial_output_dir}\\n\\n\\n\")\n        training_args.output_dir = os.path.join(training_args.output_dir, dataset_name)\n        training_args.output_dir = os.path.join(training_args.output_dir, model_name)\n        training_args.output_dir = os.path.join(training_args.output_dir, relation_representation)\n        \n        if data_args.use_context != None:\n            if data_args.use_context == \"attn_based\":\n                training_args.output_dir += \"_ac\"\n\n        # Set seed before initializing model.\n        set_seed(training_args.seed)\n\n        # Padding strategy\n        if data_args.pad_to_max_length:\n            padding = \"max_length\"\n        else:\n            # We will pad later in data collator, dynamically at batch creation, to the max sequence length in each batch\n            padding = False\n        \n        def compute_metrics(p: EvalPrediction):\n            print(f\"\\nRunning Training Predictions: {EvalPrediction}\\n\")\n            pred, true = p.predictions, p.label_ids\n\n            pred = np.argmax(pred, axis=1)\n            true = true.flatten()\n\n            pred = pred.tolist()\n            true = true.tolist()\n            \n            # Remove ignored labels.\n            # For ChemProt, ignore false labels. \"CPR:false\": \"id\": 0\n            # For DDI, ignore false labels. \"DDI-false\": \"id\": 0\n            # For TACRED, ignore no relation labels. \"no_relation\": \"id\": 0\n            if any(x == dataset_name for x in ['ChemProt_BLURB', 'DDI_BLURB', 'TACRED']):\n                cleaned_pred_true = [(p, t) for (p, t) in zip(pred, true) if t != 0]\n                pred = [x[0] for x in cleaned_pred_true]\n                true = [x[1] for x in cleaned_pred_true]\n            \n            # metrics ref: https://github.com/huggingface/datasets/tree/master/metrics\n            a_m = load_metric(\"accuracy\")\n            p_m = load_metric(\"precision\")\n            r_m = load_metric(\"recall\")\n            f_m = load_metric(\"f1\")\n            \n            if any(x == dataset_name for x in [\"GAD_BLURB\", \"EU-ADR_BioBERT\"]):\n                average = \"binary\"\n            else:\n                average = \"micro\"\n            \n            a = a_m.compute(predictions=pred, references=true)\n            p = p_m.compute(predictions=pred, references=true, average=average)\n            r = r_m.compute(predictions=pred, references=true, average=average)\n            f = f_m.compute(predictions=pred, references=true, average=average)\n            \n            return {\"accuracy\": a[\"accuracy\"], \"precision\": p[\"precision\"], \"recall\": r[\"recall\"], \"f1\": f[\"f1\"]}\n\n        # Remove old output files except the cross-validation result file.\n        if os.path.exists(training_args.output_dir):\n            for f in os.listdir(training_args.output_dir):\n                if os.path.isfile(os.path.join(training_args.output_dir, f)) and f != \"predict_results_history.json\":\n                    os.remove(os.path.join(training_args.output_dir, f))\n            \n        # Get the number of datasets. If it's more than 1, it's a cross-validation dataset.\n        num_of_datasets = 0\n        for file in os.listdir(os.path.join(data_args.dataset_dir, data_args.dataset_name)):\n            if file.startswith('train_') and file.endswith('.json'):\n                num_of_datasets += 1\n                print(f\"*** No. of datasets: {num_of_datasets} ***\")\n\n        for dataset_num in range(num_of_datasets):\n            logger.info(\"\\n\\n*** Dataset number: \" + str(dataset_num) + \" ***\\n\\n\")\n\n            # Load pretrained model and tokenizer\n            #\n            # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n            # download model & vocab.\n            config = AutoConfig.from_pretrained(\n                model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n                num_labels=len(label_list),\n                finetuning_task=task_name,\n                cache_dir=model_args.cache_dir,\n                revision=model_args.model_revision,\n                use_auth_token=True if model_args.use_auth_token else None,\n                \n                # get attention outputs for attention based context.\n                # ref: https://discuss.huggingface.co/t/output-attention-true-after-downloading-a-model/907\n                output_attentions=True if data_args.use_context == \"attn_based\" else None,\n            )\n            \n            # Explicitly set 'do_lower_case' since some models have a wrong case setting. (e.g., BioBERT, SciBERT)\n            # HuggingFace ALBERT models and PubMedBERT are uncased.\n            if config.model_type == \"albert\" or \"PubMedBERT\" in model_name:\n                do_lower_case = True \n            else:\n                do_lower_case = model_args.do_lower_case\n            \n            tokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n            # Use 'add_prefix_space' for GPT2, RoBERTa, DeBERTa\n            if config.model_type in {\"gpt2\", \"roberta\", \"deberta\", \"deberta-v2\"}:\n                tokenizer = AutoTokenizer.from_pretrained(\n                    tokenizer_name_or_path,\n                    cache_dir=model_args.cache_dir,\n                    use_fast=True,\n                    revision=model_args.model_revision,\n                    use_auth_token=True if model_args.use_auth_token else None,\n                    add_prefix_space=True,\n                )\n            else:\n                # Set 'do_lower_case' when loading a tokenizer. It's not working to change the variable after it's loaded (i.e., tokenizer.do_lower_case = False).\n                # GPT2, RoBERTa, DeBERTa don't have 'do_lower_case'. \n                tokenizer = AutoTokenizer.from_pretrained(\n                    tokenizer_name_or_path,\n                    cache_dir=model_args.cache_dir,\n                    use_fast=True,\n                    revision=model_args.model_revision,\n                    use_auth_token=True if model_args.use_auth_token else None,\n                    do_lower_case=do_lower_case,\n                )\n            \n            max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n            \n            ### TODO: delete this if not necessary (it doesn't affect the performance). 03-02-2022\n            #\n            # The default max length of BioBERT & DeBERTa is too large (1000000000000000019884624838656), so set it to 512.\n            '''\n            if isinstance(tokenizer, BertTokenizerFast) or isinstance(tokenizer, DebertaTokenizerFast) or isinstance(tokenizer, DebertaV2Tokenizer):\n                max_seq_length = 512 \n            '''\n            \n            # Remove any parent directories of the dataset if exist.\n            dataset_name = dataset_name.rsplit('/', 1)[1] if len(dataset_name.rsplit('/', 1)) > 1 else dataset_name\n            \n            # Yashfinul Haque - 27/12/23\n            logger.info(f\"\\n\\nDataset being processed:{dataset_name}\\n\\n\")\n            if dataset_name in dataset_max_seq_length.keys():\n                max_seq_length = dataset_max_seq_length[dataset_name]\n\n            # Add special tokens for entity markers.\n            #\n            # Special tokens do not need to be lowercased for uncased models because they are basically all uppercased in the datasets. 03/30/2022\n            # When special_tokens is set to False (default) in add_tokens(), it treats added tokens as normal tokens.\n            # E.g., (added tokens: '@gene$', '@disease$') tokenizer.tokenize(\"@gene$, @disease$\"), tokenizer.tokenize(\"@GENE$, @DISEASE$\")\n            #       -> the same output: ['@gene$', '@disease$']\n            # When special_tokens is set to True in add_tokens(), it treats added tokens as special tokens.\n            # E.g., (added tokens: '@gene$', '@disease$') tokenizer.tokenize(\"@gene$, @disease$\"), tokenizer.tokenize(\"@GENE$, @DISEASE$\")\n            #       -> the different output: ['@gene$', '@disease$'], ['@', 'gene', '$', ',', '@', 'disease', '$']\n            #\n            # For some reason, add_tokens(special_tokens=True) doesn't update tokenizer.all_special_ids, tokenizer.all_special_tokens, ... 03/30/2022\n            # Use add_special_tokens() instead.\n            additional_special_tokens = []\n            if relation_representation.startswith('EM'):\n               additional_special_tokens.extend(entity_marker_special_tokens['EM'])\n            \n            # Add the special tokens that are used to replace entity names (entity anonymization or dummification). E.g,. ChemProt, DDI, GAD\n            if dataset_name in dataset_special_tokens.keys():\n                additional_special_tokens.extend(dataset_special_tokens[dataset_name])\n\n            # Add additional special tokens at once. If they are added separately, then only tokens added later remain. 05/04/2022\n            if len(additional_special_tokens) > 0:\n                tokenizer.add_special_tokens({\"additional_special_tokens\": additional_special_tokens})\n          \n            model = BertForRelationClassification.from_pretrained(\n                model_args.model_name_or_path,\n                from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n                config=config,\n                cache_dir=model_args.cache_dir,\n                revision=model_args.model_revision,\n                use_auth_token=True if model_args.use_auth_token else None,\n                \n                # keyword parameters for RE\n                relation_representation=relation_representation,\n                use_context=data_args.use_context,\n                tokenizer=tokenizer,\n            )\n                \n            # Resize input token embeddings matrix of the model since new tokens have been added.\n            # this funct is used if the number of tokens in tokenizer is different from config.vocab_size.\n            model.resize_token_embeddings(len(tokenizer))\n\n            # Loading a dataset from your local files.\n            data_files = read_dataset(dataset_num, task_name, data_args)\n            \n            print(f\"\\n\\n\\nDataset Loading: {data_files}.\\n\\n\\n\")\n            \n            print(f\"\\nDataset Name: {dataset_name}.\\n\")\n            # Yashfinul Haque - 27/12/23\n            logger.info(f\"\\n\\n\\nProcessing {data_files}.\\n\\n\\n\")\n\n            # This is a temporary code.\n            # In BioInfer, some entities consist of separate tokens in a text, and the partial tokens are not properly tokenized by tokenizer.\n            # To avoid the mismatch between entity index from data file and entity index from tokenized output, add the partial tokens to the tokenzier\n            # so that the tokenizer properly catch partial tokens.\n            # E.g., \"GP IIIa\" in \"GPIIb-IIIa\", \"MEK 2\" in \"MEK1/2\"\n            if dataset_name == \"BioInfer\":\n                partial_token_list = []\n                for d in concatenate_datasets([data_files[\"train\"], data_files[\"test\"]]):\n                    if len(d['relation'][0]['entity_1_idx']) > 1:\n                        for (s, e) in d['relation'][0]['entity_1_idx']:\n                            if d['text'][s-1] != ' ' or d['text'][e] != ' ':\n                                partial_token = d['text'][s:e]\n                                partial_token_list.extend(partial_token.split())\n\n                partial_token_list = list(set(partial_token_list))\n                tokenizer.add_tokens(partial_token_list)\n\n                # Resize input token embeddings matrix of the model since new tokens have been added.\n                # this funct is used if the number of tokens in tokenizer is different from config.vocab_size.\n                model.resize_token_embeddings(len(tokenizer))\n\n            dataset = featurize_data(data_files, \n                tokenizer, \n                padding, \n                max_seq_length, \n                relation_representation, \n                data_args.use_context,\n            )\n\n            train_dataset = dataset[\"train\"]\n            print(f\"\\nTraining Dataset: {train_dataset}\\n\")\n            eval_dataset = dataset[\"validation\"] if \"validation\" in dataset else None\n            test_dataset = dataset[\"test\"]\n            \n            # Data collator\n            data_collator = DataCollatorForRelationClassification(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n            \n            # Initialize our Trainer\n            trainer = Trainer(\n                model=model,\n                #model_init=get_model,\n                args=training_args,\n                train_dataset=train_dataset if training_args.do_train else None,\n                eval_dataset=eval_dataset if training_args.do_eval else None,\n                tokenizer=tokenizer,\n                data_collator=data_collator,\n                compute_metrics=compute_metrics,\n            )\n                    \n            # Detecting last checkpoint.\n            last_checkpoint = None\n            if os.path.isdir(training_args.output_dir and training_args.do_train and not training_args.overwrite_output_dir):\n                last_checkpoint = get_last_checkpoint(training_args.output_dir)\n                if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n                    raise ValueError(\n                        f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n                        \"Use --overwrite_output_dir to overcome.\"\n                    )\n                elif last_checkpoint is not None:\n                    logger.info(\n                        f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n                        \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n                    )\n            \n            # Training\n            if training_args.do_train:\n                checkpoint = None\n                if training_args.resume_from_checkpoint is not None:\n                    checkpoint = training_args.resume_from_checkpoint\n                elif last_checkpoint is not None:\n                    checkpoint = last_checkpoint\n                train_result = trainer.train(resume_from_checkpoint=checkpoint)\n                metrics = train_result.metrics\n                max_train_samples = (\n                    data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n                )\n                metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n                \n                # Yashfinul Haque - 26/11/23\n                trainer.save_model()  # Saves the tokenizer too for easy upload\n\n                trainer.log_metrics(\"train\", metrics)\n                trainer.save_metrics(\"train\", metrics)\n                trainer.save_state()\n                \n                # After training completes, perform ONNX export\n                torch.onnx.export(\n                    model,\n                    \"To identify the regions of MCP-1 that contact its receptor, CCR2, we substituted all surface-exposed residues with alanine\",\n                    \"bert_onnx_model.onnx\",\n                    opset_version=11,  # Adjust based on ONNX Runtime compatibility\n                    do_constant_folding=True,  # Optional optimization\n                    input_names=[\"input_ids\", \"attention_mask\"],\n                    output_names=[\"logits\"],\n                    dynamic_axes={\n                        \"input_ids\": {0: \"batch_size\"},  # Handle variable batch sizes\n                        \"attention_mask\": {0: \"batch_size\"}\n                    }\n                )\n            \n                eval_info = {\"dataset_num\": dataset_num, \n                             \"seed\": training_args.seed,\n                             \"epoch\": training_args.num_train_epochs,\n                             \"per_device_train_batch_size\": training_args.per_device_train_batch_size,\n                             \"n_gpu\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n                             \"learning_rate\": training_args.learning_rate,\n                             \"warmup_ratio\": training_args.warmup_ratio,\n                             \"weight_decay\": training_args.weight_decay}\n\n            # Evaluation\n            if training_args.do_eval:\n                logger.info(\"*** Evaluate ***\")\n                \n                if eval_dataset == None:\n                    logger.info(\"***Evaluation Skipped***\")\n                    print(\"***Evaluation Skipped***\")\n                    break\n                else:\n                    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n\n                    max_eval_samples = (\n                        data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n                    )\n                    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n\n                    trainer.log_metrics(\"eval\", metrics)\n\n                    # Yashfinul Haque - 26/11/23\n                    metrics[\"eval_info\"] = eval_info\n                    trainer.save_metrics(\"eval\", metrics=metrics, combined=False)\n            # Export to ONNX\n            torch.onnx.export(\n                model,  # Model to be exported\n                train_dataset,  # Input tensors\n                \"PPI-RE-onnx.onnx\",  # Output file path\n                opset_version=11,  # Target ONNX version\n                do_constant_folding=True,  # Optimize by folding constant values\n                input_names=[\"input_ids\", \"attention_mask\"],  # Specify input names\n                output_names=[\"logits\"],  # Specify output names\n            )\n        \n            # Prediction\n            if training_args.do_predict:\n                logger.info(\"*** Predict ***\")\n\n                predictions, labels, metrics = trainer.predict(test_dataset, metric_key_prefix=\"predict\")\n                \n                trainer.log_metrics(\"predict\", metrics)\n                trainer.save_metrics(\"predict\", metrics, eval_info=eval_info)\n                \n                if data_args.save_predictions:\n                    output_predict_file = os.path.join(training_args.output_dir, f\"predict_outputs.txt\")\n                    if trainer.is_world_process_zero():\n                        predictions = np.argmax(predictions, axis=1)\n                        with open(output_predict_file, \"a\") as writer:\n                            logger.info(f\"***** Predict outputs *****\")\n                            if os.path.getsize(output_predict_file) == 0:\n                                writer.write(\"index\\tentity_1\\tentity_2\\ttext\\tprediction\\tlabel\\n\")\n                            for index, (sample, item, label) in enumerate(zip(test_dataset, predictions, labels)):\n                                item = label_list[item]\n                                \n                                # debug\n                                if sample['labels'] != label:\n                                    raise Exception(\"Label mismatch!!\")\n                                \n                                ### TODO: for now, each sample has a single relation. Handle multiple relations in a sentence later.\n                                label = label_list[label[0]]\n\n                                input_ids = sample['input_ids']\n                                sent = tokenizer.decode(input_ids)\n                                sent = sent.replace('[CLS]', '').replace('[SEP]', '').strip()\n\n                                def divide_chunks(l, n):\n                                    for i in range(0, len(l), n): \n                                        yield l[i:i+n]\n                                \n                                ### TODO: for now, each sample has a single relation. Handle multiple relations in a sentence later.\n                                e1_span_idx_list, e2_span_idx_list = list(divide_chunks(sample['relations'], 2))[0]\n\n                                e1 = ' '.join([tokenizer.decode(input_ids[s:e]) for s, e in e1_span_idx_list])\n                                e2 = ' '.join([tokenizer.decode(input_ids[s:e]) for s, e in e2_span_idx_list])\n\n                                writer.write(f\"{index}\\t{e1}\\t{e2}\\t{sent}\\t{item}\\t{label}\\n\")\n            \n\n                                \n        if training_args.do_predict:\n            # Save the prediction results in a history file.\n            result_file = os.path.join(training_args.output_dir, 'predict_results.json')\n            f = open(result_file)\n            \n            txt = ''\n            for line in f.readlines():\n                line = line.strip()\n                if line == '}{':\n                    line = '}\\n{'\n                txt += line\n            \n            txt = txt.split('\\n')\n            \n            entries = [json.loads(x) for x in txt]\n            \n            result_data = {}\n            for entry in entries:\n                for k, v in entry.items():\n                    if k in result_data:\n                        result_data[k].append(v)\n                    else:\n                        result_data[k] = [v]\n            \n            out_data = {}\n            \n            # Add a predix for average scores of CV datasets.\n            key_prefix = \"avg_\" if num_of_datasets > 1 else \"\"\n            \n            out_data[key_prefix + \"predict_f1\"] = float(np.mean(result_data[\"predict_f1\"]))\n            out_data[key_prefix + \"predict_precision\"] = float(np.mean(result_data[\"predict_precision\"]))\n            out_data[key_prefix + \"predict_recall\"] = float(np.mean(result_data[\"predict_recall\"]))\n            out_data[key_prefix + \"predict_accuracy\"] = float(np.mean(result_data[\"predict_accuracy\"]))\n            out_data[key_prefix + \"predict_loss\"] = float(np.mean(result_data[\"predict_loss\"]))\n            out_data[\"epoch\"] = result_data[\"epoch\"][0]\n            out_data[\"per_device_train_batch_size\"] = result_data[\"per_device_train_batch_size\"][0]\n            out_data[\"learning_rate\"] = str(result_data[\"learning_rate\"][0])\n            out_data[\"warmup_ratio\"] = result_data[\"warmup_ratio\"][0]\n            out_data[\"weight_decay\"] = result_data[\"weight_decay\"][0]\n            out_data[\"n_gpu\"] = result_data[\"n_gpu\"][0]\n            out_data[\"num_of_datasets\"] = num_of_datasets\n            \n            if num_of_datasets > 1:\n                trainer.log_metrics(f\"{num_of_datasets} folds cross-validation predict\", out_data)\n            \n            out_file = os.path.join(training_args.output_dir, 'predict_results_history.json')\n            with open(out_file, \"a\") as f:\n                json.dump(out_data, f, indent=4, sort_keys=True)\n            \n        logger.info(\"*** Data iterations are done.  ***\")\n        \n\nif __name__ == \"__main__\":\n    main()\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-12-27T18:15:09.745839Z","iopub.execute_input":"2023-12-27T18:15:09.746177Z","iopub.status.idle":"2023-12-27T18:15:09.775033Z","shell.execute_reply.started":"2023-12-27T18:15:09.746152Z","shell.execute_reply":"2023-12-27T18:15:09.774003Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Overwriting run_re.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Running Inference**","metadata":{}},{"cell_type":"markdown","source":"### Exporting the model to ONNX for Visualization","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}