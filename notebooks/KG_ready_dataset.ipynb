{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = \"D:/CSE498R_Resources/D3N/Dengue-Drug-Discovery-Network-D3N/Data/d3n_unfiltered/unsegmented_unfiltered.csv\" #Abstract\n",
    "sentences = \"D:/CSE498R_Resources/D3N/Dengue-Drug-Discovery-Network-D3N/Data/d3n_unfiltered/segmented_unfiltered.csv\" # Sentences\n",
    "relations = \"D:/CSE498R_Resources/D3N/Dengue-Drug-Discovery-Network-D3N/Data/d3n_processed_data/filtered.csv\" # Relations\n",
    "entities = \"D:/CSE498R_Resources/D3N/Dengue-Drug-Discovery-Network-D3N/Data/d3n_processed_data/entities.csv\" # Entities\n",
    "# den = \"D:/CSE498R_Resources/D3N/Dengue-Drug-Discovery-Network-D3N/Data/d3n_filtered/d3n.json\"\n",
    "output = \"D:/CSE498R_Resources/D3N/Dengue-Drug-Discovery-Network-D3N/Data/d3n_KG_ready/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Union, Any\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Database Imports\n",
    "from neo4j import GraphDatabase\n",
    "from typing import Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "from logging.handlers import RotatingFileHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id(*args: Optional[Union[str,int]]) -> str:\n",
    "    id_string = \"-\".join(str(arg) for arg in args if arg is not None)\n",
    "    \n",
    "    # Generate SHA-256 hash of the concatenated string\n",
    "    return hashlib.sha256(id_string.encode()).hexdigest()\n",
    "\n",
    "def clean_mesh(mesh: str) -> List[str]:\n",
    "    # Remove square brackets and extra quotes\n",
    "    mesh_cleaned = mesh.strip(\"[]\").replace(\"'\", \"\")\n",
    "    # Split by commas and strip whitespace around each term\n",
    "    mesh_terms = [term.strip() for term in mesh_cleaned.split(',')]\n",
    "    \n",
    "    return mesh_terms\n",
    "\n",
    "def clean_entity_type(type:str) -> str:\n",
    "    if type.startswith('@'):\n",
    "        return type[1:]\n",
    "    else:\n",
    "        return type\n",
    "\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        # Try full format with day (e.g., \"2020 Jan 15\")\n",
    "        return datetime.strptime(date_str, \"%Y %b %d\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Try year and month only (e.g., \"2020 Jan\")\n",
    "            return datetime.strptime(date_str, \"%Y %b\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                # Try year only (e.g., \"2020\")\n",
    "                return datetime.strptime(date_str, \"%Y\")\n",
    "            except ValueError:\n",
    "                # If format is still invalid, use regex to extract the year only\n",
    "                match = re.match(r\"(\\d{4})\", date_str)\n",
    "                if match:\n",
    "                    return datetime.strptime(match.group(1), \"%Y\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid date format for date_of_publication: {date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinterpreting Relation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations_Df = pd.read_csv(relations)\n",
    "relations_df = relations_Df[:1000]\n",
    "relations_df.to_csv(\"D:/CSE498R_Resources/D3N/Dengue-Drug-Discovery-Network-D3N/Data/relation_split.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pushing Data to KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoggerSetup:\n",
    "    \"\"\"\n",
    "    Configures logging with both console and file outputs.\n",
    "    Supports log rotation and custom formatting.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_logger(\n",
    "        logger_name: str,\n",
    "        log_dir: str,\n",
    "        max_file_size_mb: int = 10,\n",
    "        backup_count: int = 5,\n",
    "        log_level: int = logging.INFO,\n",
    "        console_output: bool = True\n",
    "    ) -> logging.Logger:\n",
    "        \"\"\"\n",
    "        Sets up a logger with both file and optional console output.\n",
    "        \n",
    "        Args:\n",
    "            logger_name: Name of the logger\n",
    "            log_dir: Directory where log files will be stored\n",
    "            max_file_size_mb: Maximum size of each log file in MB\n",
    "            backup_count: Number of backup files to keep\n",
    "            log_level: Logging level (e.g., logging.INFO, logging.DEBUG)\n",
    "            console_output: Whether to also output logs to console\n",
    "            \n",
    "        Returns:\n",
    "            logging.Logger: Configured logger instance\n",
    "        \"\"\"\n",
    "        # Create logger\n",
    "        logger = logging.getLogger(logger_name)\n",
    "        logger.setLevel(log_level)\n",
    "        \n",
    "        # Create logs directory if it doesn't exist\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        \n",
    "        # Create formatters\n",
    "        file_formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        \n",
    "        console_formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        \n",
    "        # Create and configure file handler with rotation\n",
    "        log_file_path = os.path.join(log_dir, f'{logger_name}_{datetime.now().strftime(\"%Y%m%d\")}.log')\n",
    "        file_handler = RotatingFileHandler(\n",
    "            filename=log_file_path,\n",
    "            maxBytes=max_file_size_mb * 1024 * 1024,  # Convert MB to bytes\n",
    "            backupCount=backup_count,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        file_handler.setFormatter(file_formatter)\n",
    "        file_handler.setLevel(log_level)\n",
    "        logger.addHandler(file_handler)\n",
    "        \n",
    "        # Add console handler if requested\n",
    "        if console_output:\n",
    "            console_handler = logging.StreamHandler()\n",
    "            console_handler.setFormatter(console_formatter)\n",
    "            console_handler.setLevel(log_level)\n",
    "            logger.addHandler(console_handler)\n",
    "            \n",
    "        return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(logger_name: str, log_dir: str, log_level: int = logging.WARNING, console_output: bool = True) -> logging.Logger:\n",
    "    \"\"\"Set up a logger with both file and optional console output.\"\"\"\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(log_level)\n",
    "\n",
    "    # Ensure log directory exists\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Define log format\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Set up file handler with rotation\n",
    "    log_file_path = os.path.join(log_dir, f'{logger_name}_{datetime.now().strftime(\"%Y%m%d\")}.log')\n",
    "    file_handler = RotatingFileHandler(log_file_path, maxBytes=10 * 1024 * 1024, backupCount=5)\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Optionally add console output\n",
    "    if console_output:\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setFormatter(console_formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_database(uri: str, username: str, password: str, database_name: str, \n",
    "                  log_dir: str, initial_data: Optional[Dict[str, Any]] = None, \n",
    "                  delete_if_exists: bool = False) -> None:\n",
    "    logger = setup_logger(log_dir, 'GraphDB_Setup')\n",
    "    logger.info(f\"Starting database setup for {database_name}\")\n",
    "    \n",
    "    try:\n",
    "        driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "        with driver.session() as session:\n",
    "            session.run(\"RETURN 1\")\n",
    "            logger.info(\"Database connection verified\")\n",
    "            \n",
    "        with driver.session(database=\"system\") as session:\n",
    "            result = session.run(\n",
    "                \"SHOW DATABASES WHERE name = $name\",\n",
    "                name=database_name\n",
    "            )\n",
    "            db_exists = bool(result.single())\n",
    "            \n",
    "            if db_exists and delete_if_exists:\n",
    "                logger.warning(f\"Dropping existing database: {database_name}\")\n",
    "                session.run(\"DROP DATABASE $name IF EXISTS\", name=database_name)\n",
    "                \n",
    "            if not db_exists or delete_if_exists:\n",
    "                logger.info(f\"Creating new database: {database_name}\")\n",
    "                session.run(\"CREATE DATABASE $name IF NOT EXISTS\", name=database_name)\n",
    "                \n",
    "                if initial_data:\n",
    "                    logger.info(\"Initializing database with provided data\")\n",
    "                    with driver.session(database=database_name) as db_session:\n",
    "                        if 'constraints' in initial_data:\n",
    "                            for constraint in initial_data['constraints']:\n",
    "                                db_session.run(constraint)\n",
    "                                logger.debug(f\"Created constraint: {constraint}\")\n",
    "                        \n",
    "                        if 'nodes' in initial_data:\n",
    "                            for node_query in initial_data['nodes']:\n",
    "                                db_session.run(node_query)\n",
    "                                logger.debug(\"Created node batch\")\n",
    "                        \n",
    "                        if 'relationships' in initial_data:\n",
    "                            for rel_query in initial_data['relationships']:\n",
    "                                db_session.run(rel_query)\n",
    "                                logger.debug(\"Created relationship batch\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Database setup failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'driver' in locals():\n",
    "            driver.close()\n",
    "            logger.info(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityKnowledgeGraph:\n",
    "    def __init__(self, uri: str, username: str, password: str, database_name: str, log_dir: str):\n",
    "        self.logger = setup_logger(log_dir, 'EntityKnowledgeGraph')\n",
    "        self.logger.info(f\"Initializing EntityKnowledgeGraph with database: {database_name}\")\n",
    "        \n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "            self.uri = uri\n",
    "            self.username = username\n",
    "            self.database_name = database_name\n",
    "            self.logger.info(\"Successfully established database connection\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize database connection: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def initialize_database(self, initial_data: Optional[Dict[str, Any]] = None, force_recreate: bool = False) -> None:\n",
    "        self.logger.info(f\"Initializing database. Force recreate: {force_recreate}\")\n",
    "        try:\n",
    "            setup_database(\n",
    "                uri=self.uri,\n",
    "                username=self.username,\n",
    "                password=self.password,\n",
    "                database_name=self.database_name,\n",
    "                initial_data=initial_data,\n",
    "                delete_if_exists=force_recreate\n",
    "            )\n",
    "            self.logger.info(\"Database initialization completed successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Database initialization failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def close(self):\n",
    "        self.logger.info(\"Closing database connection\")\n",
    "        try:\n",
    "            self.driver.close()\n",
    "            self.logger.info(\"Database connection closed successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error closing database connection: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_or_update_paper(self, paper_data):\n",
    "        self.logger.info(f\"Creating/updating paper with PMID: {paper_data.get('pmid')}\")\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                session.execute_write(self._create_or_update_paper, paper_data)\n",
    "            self.logger.info(f\"Successfully created/updated paper {paper_data.get('pmid')}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create/update paper: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_or_update_paper(tx, paper_data):\n",
    "        pmid = paper_data.get('pmid')\n",
    "        title = paper_data.get('title')\n",
    "        date_of_publication = paper_data.get('date_of_publication')\n",
    "        mesh_keywords = paper_data.get('mesh_keywords', [])\n",
    "        \n",
    "        query = \"\"\"\n",
    "        MERGE (p:Paper {pmid: $pmid})\n",
    "        ON CREATE SET p.title = $title, p.date_of_publication = $date_of_publication, p.mesh_keywords = $mesh_keywords\n",
    "        ON MATCH SET p.title = $title, p.date_of_publication = $date_of_publication, p.mesh_keywords = $mesh_keywords\n",
    "        \"\"\"\n",
    "        tx.run(query, pmid=pmid, title=title, date_of_publication=date_of_publication, mesh_keywords=mesh_keywords)\n",
    "\n",
    "        \n",
    "    def create_or_update_sentence(self, pmid, sentence):\n",
    "        self.logger.info(f\"Creating/updating sentence {sentence.get('sentence_id')} for paper {pmid}\")\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                session.execute_write(self._create_or_update_sentence, pmid, sentence)\n",
    "            self.logger.info(f\"Successfully created/updated sentence {sentence.get('sentence_id')}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create/update sentence: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_or_update_sentence(tx, pmid, sentence):\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Paper {pmid: $pmid})\n",
    "        MERGE (s:Sentence {sentence_id: $sentence_id, text: $text})\n",
    "        MERGE (p)-[:HAS_ENTITIES]->(s)  \n",
    "        \"\"\"\n",
    "        tx.run(query, pmid=pmid, sentence_id=sentence['sentence_id'], text=sentence['text'])\n",
    "\n",
    "    def create_or_update_entity(self, sentence_id, entity):\n",
    "        self.logger.info(f\"Creating/updating entity {entity.get('entity_id')} for sentence {sentence_id}\")\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                session.execute_write(self._create_or_update_entity, sentence_id, entity)\n",
    "            self.logger.info(f\"Successfully created/updated entity {entity.get('entity_id')}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create/update entity: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_or_update_entity(tx, sentence_id, entity):\n",
    "        clean_type = entity['type'][1:] if entity['type'].startswith('@') else entity['type']\n",
    "        query = \"\"\"\n",
    "        MATCH (s:Sentence {sentence_id: $sentence_id})\n",
    "        MERGE (e:Entity {entity_id: $entity_id, name: $name})\n",
    "        MERGE (s)-[r:%s {sent_ent_edge_id: $sent_ent_edge_id}]->(e)  \n",
    "        \"\"\" % clean_type\n",
    "        tx.run(query, \n",
    "            sentence_id=sentence_id, \n",
    "            entity_id=entity['entity_id'], \n",
    "            name=entity['name'],\n",
    "            sent_ent_edge_id=entity['sent_ent_edge_id'])\n",
    "\n",
    "    def create_or_update_relationship(self, source_id, target_id, relation_type, relation_id):\n",
    "        self.logger.info(f\"Creating/updating relationship {relation_type} between entities {source_id} and {target_id} with relation_id {relation_id}\")\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                session.execute_write(self._create_or_update_relationship, source_id, target_id, relation_type, relation_id)\n",
    "            self.logger.info(f\"Successfully created/updated relationship {relation_type}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create/update relationship: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_or_update_relationship(tx, source_id, target_id, relation_type, relation_id):\n",
    "        query = \"\"\"\n",
    "        MATCH (e1:Entity {entity_id: $source_id})\n",
    "        MATCH (e2:Entity {entity_id: $target_id})\n",
    "        MERGE (e1)-[r:%s {relation_id: $relation_id}]->(e2)\n",
    "        \"\"\" % relation_type\n",
    "        tx.run(query, \n",
    "            source_id=source_id, \n",
    "            target_id=target_id, \n",
    "            relation_id=relation_id)\n",
    "\n",
    "    # Delete methods with logging\n",
    "    def delete_paper(self, pmid):\n",
    "        self.logger.info(f\"Deleting paper with PMID: {pmid}\")\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                session.execute_write(self._delete_paper, pmid)\n",
    "            self.logger.info(f\"Successfully deleted paper {pmid}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to delete paper: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    # Delete methods with logging\n",
    "    def delete_paper(self, pmid):\n",
    "        self.logger.info(f\"Deleting paper with PMID: {pmid}\")\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                session.execute_write(self._delete_paper, pmid)\n",
    "            self.logger.info(f\"Successfully deleted paper {pmid}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to delete paper: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _delete_paper(tx, pmid):\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Paper {pmid: $pmid})\n",
    "        DETACH DELETE p\n",
    "        \"\"\"\n",
    "        tx.run(query, pmid=pmid)\n",
    "\n",
    "    def delete_sentence(self, sentence_id):\n",
    "        self.logger.info(f\"Deleting sentence: {sentence_id}\")\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                session.execute_write(self._delete_sentence, sentence_id)\n",
    "            self.logger.info(f\"Successfully deleted sentence {sentence_id}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to delete sentence: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _delete_sentence(tx, sentence_id):\n",
    "        query = \"\"\"\n",
    "        MATCH (s:Sentence {sentence_id: $sentence_id})\n",
    "        DETACH DELETE s\n",
    "        \"\"\"\n",
    "        tx.run(query, sentence_id=sentence_id)\n",
    "\n",
    "    def delete_entity(self, entity_id):\n",
    "        self.logger.info(f\"Deleting entity: {entity_id}\")\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                session.execute_write(self._delete_entity, entity_id)\n",
    "            self.logger.info(f\"Successfully deleted entity {entity_id}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to delete entity: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _delete_entity(tx, entity_id):\n",
    "        query = \"\"\"\n",
    "        MATCH (e:Entity {entity_id: $entity_id})\n",
    "        DETACH DELETE e\n",
    "        \"\"\"\n",
    "        tx.run(query, entity_id=entity_id)\n",
    "\n",
    "    def delete_relationship(self, source_id, target_id, relation_type):\n",
    "        self.logger.info(f\"Deleting relationship {relation_type} between entities {source_id} and {target_id}\")\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                session.execute_write(self._delete_relationship, source_id, target_id, relation_type)\n",
    "            self.logger.info(f\"Successfully deleted relationship {relation_type}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to delete relationship: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _delete_relationship(tx, source_id, target_id, relation_type):\n",
    "        query = \"\"\"\n",
    "        MATCH (e1:Entity {entity_id: $source_id})-[r:ENTITY_RELATION {type: $relation_type}]->(e2:Entity {entity_id: $target_id})\n",
    "        DELETE r\n",
    "        \"\"\"\n",
    "        tx.run(query, \n",
    "            source_id=source_id, \n",
    "            target_id=target_id, \n",
    "            relation_type=relation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_samples(csv_path1, csv_path2, csv_path3, csv_path4):\n",
    "    # Step 1: Read the CSV files into dataframes\n",
    "    df_abstracts = pd.read_csv(csv_path1)  # Contains abstract level information\n",
    "    df_sentences = pd.read_csv(csv_path2)  # Contains sentence level information\n",
    "    df_entities = pd.read_csv(csv_path3)   # Contains entity and relation information\n",
    "    df_relations = pd.read_csv(csv_path4)\n",
    "    \n",
    "    # Step 2: Initialize the samples list\n",
    "    samples = []\n",
    "    \n",
    "    # Step 3: Iterate over abstracts to create abstract dicts\n",
    "    for _, abstract_row in tqdm(df_abstracts.iterrows(), total=len(df_abstracts), desc=\"Processing Abstracts\"):\n",
    "        dop_str = parse_date(date_str=abstract_row['Date of Publication'])\n",
    "        pmid_int = int(abstract_row['PMID'])\n",
    "        abstract = {\n",
    "            \"pmid\": pmid_int,  # Example column in csv1\n",
    "            \"title\": abstract_row['Title'],  # Example column in csv1\n",
    "            \"date_of_publication\": dop_str,  # Example column in csv1\n",
    "            \"mesh_keywords\": clean_mesh(str(abstract_row['MeSH'])),  # Example column in csv1\n",
    "            \"sentences\": []  # List to store sentence dicts\n",
    "        }\n",
    "        # abstracts.append(abstracts)\n",
    "        abstract_sentences = df_sentences[df_sentences['pmid'] == abstract_row['PMID']]\n",
    "        \n",
    "        for _, sentence_row in abstract_sentences.iterrows():\n",
    "            sent_id = generate_id(pmid_int,int(sentence_row['sent_no']))\n",
    "            sentence = {\n",
    "                \"sentence_id\": sent_id,  # Example column in csv2\n",
    "                \"text\": sentence_row['sentence'],  # Example column in csv2\n",
    "                \"entities\": [],  # List to store entity dicts\n",
    "                \"relationships\": []  # List to store relation dicts\n",
    "            }\n",
    "            \n",
    "            # Step 5: Get entities and relationships for this sentence\n",
    "            sentence_entities = df_entities[(df_entities['pmid'] == abstract_row['PMID']) & \n",
    "                                            (df_entities['sentence_no'] == sentence_row['sent_no'])]\n",
    "\n",
    "            for _, entity_row in sentence_entities.iterrows():\n",
    "                entity_type = clean_entity_type(entity_row['entity_type'])\n",
    "                sent_ent_id = generate_id(sent_id,entity_row['entity_name'],entity_type)\n",
    "                entity = {\n",
    "                    \"entity_id\": generate_id(entity_row['entity_name'],entity_type),  # Example column in csv3\n",
    "                    \"name\": entity_row['entity_name'],  # Example column in csv3\n",
    "                    \"type\": entity_type,  # Example column in csv3\n",
    "                    \"sent_ent_edge_id\":sent_ent_id\n",
    "                }\n",
    "                sentence['entities'].append(entity)\n",
    "                \n",
    "            # Step 6: Get relations based on entities\n",
    "            sentence_relations = df_relations[(df_relations['pmid'] == abstract_row['PMID']) & \n",
    "                                              (df_relations['sent_no'] == sentence_row['sent_no'])]\n",
    "\n",
    "            if not sentence_relations.empty:\n",
    "                for _, relation_row in sentence_relations.iterrows():\n",
    "                    entity_1_type = clean_entity_type(relation_row['entity_1_type'])\n",
    "                    entity_2_type = clean_entity_type(relation_row['entity_2_type'])\n",
    "                    sei = generate_id(relation_row['entity_1'],entity_1_type)\n",
    "                    tei = generate_id(relation_row['entity_2'],entity_2_type)\n",
    "                    relation = {\n",
    "                        \"source_entity_id\": sei,\n",
    "                        \"target_entity_id\": tei,\n",
    "                        \"relation_id\": generate_id(sei,tei, relation_row['relation_type']),\n",
    "                        \"source_entity_name\": relation_row['entity_1'],\n",
    "                        \"target_entity_name\": relation_row['entity_2'],\n",
    "                        \"relation_type\": relation_row['relation_type']\n",
    "                    }\n",
    "                    sentence['relationships'].append(relation)\n",
    "            else:\n",
    "                # Append an empty dictionary with attributes but no values if no relations are found\n",
    "                relation = {\n",
    "                    \"source_entity_id\": None,\n",
    "                    \"target_entity_id\": None,\n",
    "                    \"relation_type\": None\n",
    "                }\n",
    "                sentence['relationships'].append(relation)    \n",
    "            \n",
    "            # Append the sentence dict to the abstract's sentences list\n",
    "            abstract['sentences'].append(sentence)\n",
    "        \n",
    "        # Append the abstract dict to the samples list\n",
    "        samples.append(abstract)\n",
    "    \n",
    "    return samples\n",
    "\n",
    "samples = convert_to_samples(abstracts,sentences,entities,relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': 30496157,\n",
       " 'title': 'Outbreak of Dengue Virus Type 2 - American Samoa, November 2016-October 2018.',\n",
       " 'date_of_publication': datetime.datetime(2018, 11, 30, 0, 0),\n",
       " 'mesh_keywords': ['Adolescent',\n",
       "  'Adult',\n",
       "  'Aged',\n",
       "  'Aged',\n",
       "  '80 and over',\n",
       "  'American Samoa/epidemiology',\n",
       "  'Child',\n",
       "  'Child',\n",
       "  'Preschool',\n",
       "  'Dengue/*epidemiology/*virology',\n",
       "  'Dengue Virus/classification/genetics/*isolation & purification',\n",
       "  '*Disease Outbreaks',\n",
       "  'Female',\n",
       "  'Humans',\n",
       "  'Infant',\n",
       "  'Infant',\n",
       "  'Newborn',\n",
       "  'Male',\n",
       "  'Middle Aged',\n",
       "  'Young Adult'],\n",
       " 'sentences': [{'sentence_id': 'c6e0a01936b45a227524431f868924f0469861cbf54b9645945a2c6f7c47c027',\n",
       "   'text': 'The U.S. territory of American Samoa has experienced recent outbreaks of illnesses caused by viruses transmitted by Aedes species mosquitoes, including dengue, chikungunya, and Zika virus.',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': '656147c006c8fb0e275cae5d1dcf72d3ef751332bcf83d8fab0eda54e151fb00',\n",
       "   'text': 'In November 2016, a traveler from the Solomon Islands tested positive for infection with dengue virus type 2 (DENV-2).',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': '039eeeef67e3b643c8417bf726f298b2ba234ff15e5412a861fcc991bb3956b0',\n",
       "   'text': 'Additional dengue cases were identified in the subsequent weeks through passive and active surveillance.',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': '3263db930b8717db9ffaa8cb29a60b6e969ed3c5406d8651177430da37abc3a0',\n",
       "   'text': 'Suspected dengue cases were tested locally with a dengue rapid diagnostic test (RDT) for DENV nonstructural protein 1 (NS1).',\n",
       "   'entities': [{'entity_id': 'f52b238596ee845b0126d488bd4029e5010c02e9146800d0f74bef4631eb2933',\n",
       "     'name': 'DENV nonstructural protein 1',\n",
       "     'type': 'PROTEIN',\n",
       "     'sent_ent_edge_id': 'e2eee69a401ad1da52d3523a35407a992591e72ee0718c1df19c60315bd56303'}],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': 'c15a154c629dc649f88c03c7a4e03eeae3fd6728936c62612e82f70ceb11178b',\n",
       "   'text': 'Specimens from RDT-positive cases and patients meeting the dengue case definition were tested by real-time reverse transcription-polymerase chain reaction (real-time RT-PCR) at Hawaii State Laboratories.',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': '8763bb34cd6e7d20463b715a6d9ab57f6fa2c53b7fbca245d1872abe73226a1d',\n",
       "   'text': 'During November 2016-October 2018, a total of 3,240 patients were tested for evidence of DENV infection (118 by RDT-NS1 alone, 1,089 by real-time RT-PCR alone, and 2,033 by both methods), 1,081 (33.4%) of whom tested positive for dengue (19.5 per 1,000 population).',\n",
       "   'entities': [{'entity_id': '8a1aefa2aba7f463dcf03ae77d7a7654fcb252e890cf3c1bd7b7eafa898b8a6e',\n",
       "     'name': 'RDT-NS1',\n",
       "     'type': 'PROTEIN',\n",
       "     'sent_ent_edge_id': '0ada33a368db2c04f22641ae836f6e29d634509a902057b91df2818771dd398b'}],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': '967f415301c2b8a2d74fad6c76f507dc8d47cfc7a0c571333b0f9442d998fe9a',\n",
       "   'text': 'All 941 real-time RT-PCR-positive specimens were positive for DENV-2.',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': 'f935edbad08d5aab2be1b38f2a92f53c04be61c4bc5a93387fb107b6f481b299',\n",
       "   'text': 'The monthly number of laboratory-confirmed cases peaked at 120 during December 2017.',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': 'd43da95285d7f3cf88263e54a0fdaf92e29d777008d32f3587b65dbc596c2887',\n",
       "   'text': 'Among laboratory-confirmed dengue cases, 380 (35.2%) patients were hospitalized; one patient, who was transferred to American Samoa for care late in his illness, died.',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': 'be0ec19dd4a5262ce2231b9d60fc739bf997b493cb1907d89e858595cc2b9d34',\n",
       "   'text': 'The public health response to this outbreak included disposal of solid waste to remove mosquito breeding sites, indoor residual spraying of pesticides in schools, reinforcement of dengue patient management education, and public education on mosquito avoidance and seeking medical care for symptoms of dengue.',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = samples[1600:1700]\n",
    "s[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def create_D3N(papers_data: List[Dict[str, Any]], graph: 'EntityKnowledgeGraph', log_dir: str) -> None:\n",
    "    \"\"\"Create or update a paper with sentences, entities, and relationships in a Neo4j graph database.\"\"\"\n",
    "    \n",
    "    # Initialize logger\n",
    "    logger = setup_logger('D3N_Creator', log_dir)\n",
    "    \n",
    "    for data in tqdm(papers_data,total=len(papers_data), desc=\"Pushing to the KG\"):  # Directly iterate over the list of Document instances\n",
    "        try:\n",
    "            pmid = int(data.get('pmid')) # Access pmid directly from the Document instance\n",
    "            logger.info(f\"Processing paper with PMID: {pmid}\")\n",
    "\n",
    "            # Create or update paper entry in the graph\n",
    "            graph.create_or_update_paper({\n",
    "                'pmid': pmid,\n",
    "                'title': data.get('title'),\n",
    "                'date_of_publication': data.get('date_of_publication'),\n",
    "                'mesh_keywords': data.get('mesh_keywords', [])\n",
    "            })\n",
    "\n",
    "            # Step 2: Create or update sentences, entities, and relationships\n",
    "            for sentence in data.get('sentences', []):  # Access sentences directly\n",
    "                sentence_id = sentence.get('sentence_id')\n",
    "                if not sentence_id or not sentence.get('entities'):\n",
    "                    continue\n",
    "\n",
    "                # Update sentence with primary key `pmid` and sentence data\n",
    "                graph.create_or_update_sentence(pmid, {\n",
    "                    'sentence_id': sentence_id,\n",
    "                    'text': sentence.get('text')\n",
    "                })\n",
    "\n",
    "                # Process entities and relationships within the sentence\n",
    "                for entity in sentence.get('entities', []):  # Iterate over entities in the sentence\n",
    "                    graph.create_or_update_entity(sentence_id, {\n",
    "                        'entity_id': entity.get('entity_id'),\n",
    "                        'name': entity.get('name'),\n",
    "                        'type': entity.get('type'),\n",
    "                        'sent_ent_edge_id': entity.get('sent_ent_edge_id')\n",
    "                    })\n",
    "\n",
    "                # Process relationships between entities\n",
    "                for relationship in sentence.get('relationships', []):  # Iterate over relationships in the sentence\n",
    "                    source_id = relationship.get('source_entity_id')\n",
    "                    target_id = relationship.get('target_entity_id')\n",
    "                    relation_type = relationship.get('relation_type')\n",
    "                    relation_id = relationship.get('relation_id')\n",
    "                    \n",
    "                    if source_id and target_id and relation_type and relation_id:\n",
    "                        graph.create_or_update_relationship(\n",
    "                            source_id=source_id,\n",
    "                            target_id=target_id,\n",
    "                            relation_type=relation_type,\n",
    "                            relation_id=relation_id\n",
    "                        )\n",
    "\n",
    "            logger.info(f\"Successfully processed paper with PMID: {pmid}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process paper with PMID {data.get('pmid')}: {str(e)}\")\n",
    "        \n",
    "    logger.info(\"Data push to knowledge graph completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pmid': 34316455,\n",
       " 'title': 'Study of the Langat virus RNA-dependent RNA polymerase through homology modeling.',\n",
       " 'date_of_publication': datetime.datetime(2021, 1, 1, 0, 0),\n",
       " 'mesh_keywords': ['nan'],\n",
       " 'sentences': [{'sentence_id': 'd0571027b27d97ff346ec55ffbd6b1710489518a847da54c33f098fc7e39a77b',\n",
       "   'text': 'Langat virus is a member of the Flaviviridae family and a close relative of a group of important tick-borne viruses that cause human encephalitis.',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': 'c111ebbccb62d1048e17c2e3e1d2254b7091e807b240654ff7ffa821c7b76244',\n",
       "   'text': 'RNA-dependent RNA polymerase is a significant component of the replication mechanism of the Flaviviridae viral family.',\n",
       "   'entities': [{'entity_id': 'da25e8463473555a7534576ff559ff7c96ed9fcba082c871c95815b56bbb6684',\n",
       "     'name': 'RNA-dependent RNA polymerase',\n",
       "     'type': 'PROTEIN',\n",
       "     'sent_ent_edge_id': '11fd38a95ce684032eab712260894dd4011db91ac70eedcc7e5c90b3e9060283'}],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': 'f978475dbcce5864f073ff420d640e9c7fcf24ba93f912bcdd4bbdf9d4cfab64',\n",
       "   'text': 'In the present work, a three-dimensional model of the Langat virus RNA-dependent RNA polymerase was designed through homology modeling.',\n",
       "   'entities': [{'entity_id': 'c65daaacf004963b4d20b2522e4eccc54a41bd683f67ca6f2bce494c36bc7a0c',\n",
       "     'name': 'Langat virus RNA-dependent RNA polymerase',\n",
       "     'type': 'PROTEIN',\n",
       "     'sent_ent_edge_id': '1f0ba3ae40bbff0de3af577c67b4260e1a6e806e2d655c61284d237853e32f26'}],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': '3d34fc5652ab7f2b638b8989eb0b071f60b9dbf5250382488238074a076bbf46',\n",
       "   'text': 'The experimentally determined structure of the RNA-dependent RNA polymerase of Dengue virus type II, another member of the same viral family, was employed as template for the homology modeling process.',\n",
       "   'entities': [{'entity_id': 'da25e8463473555a7534576ff559ff7c96ed9fcba082c871c95815b56bbb6684',\n",
       "     'name': 'RNA-dependent RNA polymerase',\n",
       "     'type': 'PROTEIN',\n",
       "     'sent_ent_edge_id': '51f358956202fddced7d7556027f2ea36c4c8b84b0cae42555e720403adfd850'}],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': 'a9daa07cfd40cb9614dc540f1b669ae3e9b799852e55f437a10daef599c45dcc',\n",
       "   'text': 'The resulting model underwent a series of optimisations and its quality was verified using the Verify3D algorithm.',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': 'eb0224dba736aa33a34aee7d2a63608896c8ab3ffe86822fe689c38359a38e08',\n",
       "   'text': 'Important functional characteristics of the family of viral RNA-dependent RNA polymerases were identified in the generated model, thus affirming the potential for its use in the possible design of anti-viral agents for Langat virus. and Biotechnology, Agricultural University of Athens, Athens, Greece. and Biotechnology, Agricultural University of Athens, Athens, Greece. and Biotechnology, Agricultural University of Athens, Athens, Greece. and Biotechnology, Agricultural University of Athens, Athens, Greece. University of Patras, Patras, Greece. Hellenic Agricultural Organization - Demeter, Attica, Greece. and Biotechnology, Agricultural University of Athens, Athens, Greece. Translational Research, Biomedical Research Foundation of the Academy of Athens, Athens, Greece. College London, London, United Kingdom.',\n",
       "   'entities': [{'entity_id': '13bafa3270e72f9e39faeeeb89b194465e5f14e3e9996b390daf2571d8aae62f',\n",
       "     'name': 'viral RNA-dependent RNA polymerases',\n",
       "     'type': 'PROTEIN',\n",
       "     'sent_ent_edge_id': 'ac674ca792655820d37f27f9f6319bdddfa0c61356899c0e76a45b72c064a33d'}],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]},\n",
       "  {'sentence_id': 'bfae31c0fafa916820666b6ade9beca35c947c359c76b91e300f42af2dce8555',\n",
       "   'text': 'none',\n",
       "   'entities': [],\n",
       "   'relationships': [{'source_entity_id': None,\n",
       "     'target_entity_id': None,\n",
       "     'relation_type': None}]}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DO NOT USE IT IF YOU HAVE ALREADY EXECUTED THE PREVIOUS CELL.\n",
    "\"\"\"\n",
    "s = samples[8000:]\n",
    "print(len(s))\n",
    "s[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing to the KG: 100%|██████████| 1559/1559 [20:39<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize components with logging\n",
    "log_dir = \"D:/CSE498R_Resources/D3N/Dengue-Drug-Discovery-Network-D3N/Logs/\"\n",
    "setup_database(uri=\"bolt://localhost:7687\", \n",
    "              username=\"neo4j\",\n",
    "              password=\"adminPassword\",\n",
    "              database_name=\"D3N\",\n",
    "              log_dir=log_dir)\n",
    "# Initialize graph\n",
    "graph = EntityKnowledgeGraph(uri=\"bolt://localhost:7687\", \n",
    "              username=\"neo4j\",\n",
    "              password=\"adminPassword\",\n",
    "              database_name=\"D3N\",\n",
    "              log_dir=log_dir)\n",
    "\n",
    "create_D3N(s, graph,log_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d3n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
